{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_feature_path = \"./datasets/train/train_feature.npz\"\n",
    "valid_feature_path = \"./datasets/valid/valid_feature.npz\"\n",
    "test_feature_path = \"./datasets/test/test_feature.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "data = np.load(train_feature_path, allow_pickle=True)\n",
    "valid_data = np.load(valid_feature_path, allow_pickle=True)\n",
    "test_data = np.load(test_feature_path, allow_pickle=True)\n",
    "\n",
    "train_deep_X = data['features']\n",
    "train_deep_Y = data['label']\n",
    "\n",
    "valid_deep_X = valid_data['features']\n",
    "valid_deep_Y = valid_data['label']\n",
    "\n",
    "\n",
    "train_X_deep_flattened = train_deep_X.reshape(train_deep_X.shape[0], -1)\n",
    "valid_X_deep_flattened = valid_deep_X.reshape(valid_deep_X.shape[0], -1)\n",
    "print(train_X_deep_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of components you want to reduce to\n",
    "n_components = 500\n",
    "pca = PCA(n_components=n_components)\n",
    "train_embeddings_pca = pca.fit_transform(train_X_deep_flattened)\n",
    "validation_embeddings_pca = pca.transform(valid_X_deep_flattened)\n",
    "\n",
    "explained_variance_ratio = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Total retained variance by {n_components} components: {explained_variance_ratio * 100:.2f}%\")\n",
    "\n",
    "print(f\"Train Embeddings Shape after PCA: {train_embeddings_pca.shape}\")\n",
    "print(f\"Validation Embeddings Shape after PCA: {validation_embeddings_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Dictionary of models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),\n",
    "    \"Ridge Regression\":RidgeClassifier(),\n",
    "#     \"Linear Discriminant\" : LinearDiscriminantAnalysis(),\n",
    "    \"XG Boost\" : XGBClassifier(),\n",
    "#     \"Naive Bayes\" : GaussianNB(priors=None, var_smoothing=1.0),\n",
    "    \"SVM\": SVC(C= 0.01, degree=3, gamma='scale', kernel= 'linear'),\n",
    "}\n",
    "\n",
    "percentages = np.linspace(0.2, 1.0, 5)\n",
    "results = []\n",
    "for percent in percentages:\n",
    "    n_samples = int(len(train_embeddings_pca) * percent)\n",
    "    print(f\"\\nTraining with {percent*100:.1f}% of the data ({n_samples} samples)\")\n",
    "\n",
    "    # Select a subset of training data based on the percentage\n",
    "    X_subset = train_embeddings_pca[:n_samples]\n",
    "    y_subset = train_deep_Y[:n_samples]\n",
    "\n",
    "    # Train and evaluate models\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_subset, y_subset)\n",
    "\n",
    "        y_pred_valid = model.predict(validation_embeddings_pca)\n",
    "        validation_accuracy = accuracy_score(valid_deep_Y, y_pred_valid)\n",
    "        print(f\"Validation accuracy for {model_name}: {validation_accuracy}\")\n",
    "        results.append([percent * 100, model_name, validation_accuracy])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(results, columns=['percent', 'model_name', 'validation_accuracy'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for model_name in df['model_name'].unique():\n",
    "    model_df = df[df['model_name'] == model_name]\n",
    "    plt.plot(model_df['percent'], model_df['validation_accuracy'], marker='o', label=model_name)\n",
    "\n",
    "plt.title('Validation Accuracy vs. Percentage of Data Used (With Hyperparameter Tuning)')\n",
    "plt.xlabel('Percentage of Data Used (%)')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend(title='Model Name')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
